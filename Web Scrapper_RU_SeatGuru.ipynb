{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen as uReq\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to deal with empty elements when extracting the first element of a list\n",
    "def helper(x):\n",
    "    if len(x)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return x[0]\n",
    "    \n",
    "# A helper function to output a soup object using url\n",
    "def return_soup(url):\n",
    "    uClient = uReq(url)\n",
    "    html = uClient.read()\n",
    "    res_soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    return res_soup\n",
    "\n",
    "# A helper function to extract amenities\n",
    "# Input: a list of strings, a word\n",
    "# Output: a list of boolean values denoting whether each entry has that word\n",
    "# Eg helper_amen(['a','ab','c'], 'a') = [True, True, False]\n",
    "def helper_amen(li, word):\n",
    "    res = [word in str(s) for s in li]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to read PLANE-specific pages\n",
    "def from_plane_soup(soup):\n",
    "    \n",
    "    # Find the comment section (return nothing if there is no comment)\n",
    "    try:\n",
    "        soup_comment_section = soup.find(class_=\"comment-box\")\n",
    "        \n",
    "        # Extract html code for dates, names, comments, plane names\n",
    "        name_plane = soup.findAll(class_=\"h1-fix\")[0].get_text()\n",
    "        dates_seats = soup_comment_section.findAll(class_=\"date\")\n",
    "        names = soup_comment_section.findAll(class_=\"name\")\n",
    "        comments = soup_comment_section.findAll(class_=\"comment\")\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    # Clearn up what we have above\n",
    "    dates = [helper(re.findall('[0-9]{4}\\/[0-9]{2}\\/[0-9]{2}', s.get_text())) for s in dates_seats]\n",
    "    seats = [helper(re.findall('Seat ([A-Z0-9]*) ', s.get_text())) for s in dates_seats]\n",
    "    names = [helper([s.get_text().strip()]) for s in names]\n",
    "    comments = [helper([s.get_text().strip()]) for s in comments]\n",
    "\n",
    "    # Put everything into a dataframe\n",
    "    df = pd.DataFrame({'Date':dates,\n",
    "                       'Plane':name_plane,\n",
    "                       'Seat':seats, \n",
    "                       'Name':names,\n",
    "                       'Comment':comments})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to read COMPANY-specific pages\n",
    "def from_company_soup(soup):\n",
    "    # Extract html code for plane categories, amenities and urls\n",
    "    categories = soup.findAll(class_=\"chartsTitle\")[1:]\n",
    "    planes = soup.findAll(class_=\"seats\")\n",
    "\n",
    "    # Clearn up what we have above\n",
    "    categories = [s.find(\"h3\").get_text()[3:] for s in categories]\n",
    "    urls = [s.findAll(class_=\"aircraft_seats\") for s in planes]\n",
    "    urls = [[s2.find('a', href=True)['href'] for s2 in s ] for s in urls]\n",
    "    amenities = [s.findAll(id='amenities_list') for s in planes]\n",
    "\n",
    "    # Put everything into a dataframe\n",
    "    df = pd.DataFrame({'Url':urls,\n",
    "                       'Category':categories})\n",
    "    df = df.explode('Url')\n",
    "\n",
    "    # Deal with amenities\n",
    "    df['temp'] = [b for a in amenities for b in a]\n",
    "    for amen in ['food', 'wifi', 'tv', 'headphones', 'elec']:\n",
    "        df[amen.capitalize()] = helper_amen(df['temp'].tolist(), amen)\n",
    "    df = df.drop('temp', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to read the base page where all companies are listed\n",
    "def from_base_soup(soup):\n",
    "    # Find the airline section\n",
    "    soup_airline_section = soup.findAll(class_=\"browseAirlines\")[0]\n",
    "\n",
    "    # Find urls\n",
    "    urls = soup_airline_section.findAll('a', href=True)\n",
    "\n",
    "    # Clearn up what we have above\n",
    "    urls = [s['href'] for s in urls]\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to obtain an additional table using COMPANY-specific pages\n",
    "def table_from_company_soup(soup):\n",
    "    # Find the link of the table\n",
    "    url_table = soup.find('a', string='Compare seat pitch, etc.')['href']\n",
    "    soup_table = return_soup('https://www.seatguru.com' + url_table)\n",
    "\n",
    "    # Obtain a list of dfs and its corresponding class types\n",
    "    list_df = pd.read_html('https://www.seatguru.com' + url_table)\n",
    "    list_class = soup_table.findAll(class_='class-of-service')\n",
    "\n",
    "    # Clearn up what we have above\n",
    "    list_class = [s.get_text() for s in list_class]\n",
    "    for n in range(len(list_df)):\n",
    "        list_df[n]['Class'] = list_class[n]\n",
    "\n",
    "    # Put everything into a dataframe\n",
    "    df = pd.concat(list_df,axis=0,ignore_index=True)\n",
    "    df['Company'] = url_table.split('/')[2].replace('_', ' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin scrapping comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the base page\n",
    "url = 'https://www.seatguru.com/browseairlines/browseairlines.php'\n",
    "list_urls_companies = from_base_soup(return_soup(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each company\n",
    "res = []\n",
    "for url_company in list_urls_companies:\n",
    "    url_company = 'https://www.seatguru.com' + url_company\n",
    "    res.append(from_company_soup(return_soup(url_company)))\n",
    "    \n",
    "    # Tracker\n",
    "    if len(res) % 5 == 0:\n",
    "        print(len(res), 'companies collected')\n",
    "    \n",
    "df_company = pd.concat(res,axis=0,ignore_index=True)\n",
    "# df_company.to_pickle('df_company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each plane for each company\n",
    "\n",
    "# A helper function to handle the map\n",
    "def helper_map(url):\n",
    "    url_plane = 'https://www.seatguru.com' + url\n",
    "    df = from_plane_soup(return_soup(url_plane))\n",
    "    \n",
    "    # Return nothing if there is no comment\n",
    "    if df is None:\n",
    "        return None\n",
    "    else:\n",
    "        df['Url'] = url\n",
    "        return df\n",
    "\n",
    "df_company = pd.read_pickle('df_company')\n",
    "res = df_company['Url'].map(helper_map)\n",
    "df_plane = pd.concat(res.to_list(),axis=0,ignore_index=True)\n",
    "# df_plane.to_pickle('df_plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two dfs together!\n",
    "df_company = pd.read_pickle('df_company')\n",
    "df_plane = pd.read_pickle('df_plane')\n",
    "df = df_plane.merge(df_company, on=['Url'])\n",
    "#df.to_pickle('df_seatguru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin scrapping class tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the base page\n",
    "url = 'https://www.seatguru.com/browseairlines/browseairlines.php'\n",
    "list_urls_companies = from_base_soup(return_soup(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 companies collected\n",
      "10 companies collected\n",
      "15 companies collected\n",
      "20 companies collected\n",
      "25 companies collected\n",
      "30 companies collected\n",
      "35 companies collected\n",
      "40 companies collected\n",
      "45 companies collected\n",
      "50 companies collected\n",
      "55 companies collected\n",
      "60 companies collected\n",
      "65 companies collected\n",
      "70 companies collected\n",
      "75 companies collected\n",
      "80 companies collected\n",
      "85 companies collected\n",
      "90 companies collected\n",
      "95 companies collected\n",
      "100 companies collected\n",
      "105 companies collected\n",
      "110 companies collected\n",
      "115 companies collected\n",
      "120 companies collected\n",
      "125 companies collected\n",
      "130 companies collected\n",
      "135 companies collected\n",
      "140 companies collected\n",
      "145 companies collected\n",
      "150 companies collected\n",
      "155 companies collected\n",
      "160 companies collected\n",
      "165 companies collected\n",
      "170 companies collected\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each company\n",
    "res = []\n",
    "for url_company in list_urls_companies:\n",
    "    url_company = 'https://www.seatguru.com' + url_company\n",
    "    res.append(table_from_company_soup(return_soup(url_company)))\n",
    "    \n",
    "    # Tracker\n",
    "    if len(res) % 5 == 0:\n",
    "        print(len(res), 'companies collected')\n",
    "    \n",
    "df_class = pd.concat(res,axis=0,ignore_index=True)\n",
    "# df_class.to_pickle('df_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
